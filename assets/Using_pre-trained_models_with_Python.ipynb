{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing [Huggingface](https://huggingface.co/)  ðŸ¤—\n",
    "\n",
    "## Resource for pre-trained models, data, and more\n",
    "\n",
    "<span style=\"color:red\">Disclaimer: Using models will download model weights onto your machine!</span>\n",
    "\n",
    "Note: Distilled models are downsized clones of the original model that often achieve comparable performance. For an explanation of the process, see for example [this post](https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells only need to be executed once.\n",
    "To run them, remove the # in the code cells below and execute them.\n",
    "\n",
    "The ! tells the notebook to execute the following on the command line.\n",
    "\n",
    "First, install the requirements with Python's package manager (pip3).\n",
    "\n",
    "Huggingface requires either [pytorch](https://pytorch.org/) or [tensorflow](https://www.tensorflow.org/) as deep learning backend. We will use pytorch here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --user torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitfall: Models can also be pre-trained using either pytorch or tensorflow as backend. The resulting weight files will be in different formats!\n",
    "Huggingface provides an option to use them with either backend, but this requires both backends to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --user tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, install the huggingface library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --user transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python loads modules using the **import** statement.\n",
    "We will use the pytorch backend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For the sake of completeness, we will import all the neccessary _transformer_ \n",
    "packages (libraries/modules) in each section. \n",
    "Usually it is enough to import each of them only once within the notebook or script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained models\n",
    "* Browse pre-trained models on https://huggingface.co/models\n",
    "* There are official models trained by the huggingface people and user-built models (everyone can upload a model)\n",
    "* Filter available models by\n",
    " * tasks\n",
    " * languages\n",
    " * training data\n",
    " * DL framework\n",
    "* Find additional information (also on limitations) in model card\n",
    " * Check model card content and model \"popularity\" as a metric for usability\n",
    "* Test model on Hosted Inference API in browser (at least for some of them)\n",
    "* Get examples on how to use them\n",
    "* Check [model class documentation](https://huggingface.co/docs/transformers/model_doc/auto) for further details \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipelines\n",
    "\n",
    "* Pipelines provide black-box wrappers for many tasks for very easy use\n",
    "* See [pipelines documentation](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/pipelines) for further details on how to use them\n",
    "\n",
    "In the following, we will first see how to use the pipeline wrapper and then look at what actually happens inside it for a few example tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Using pre-trained models for text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Generate a cloze-style probability distribution for a masked word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloze-style word prediction using masked language models (BERT & co.) with pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening within this pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input\n",
    "\n",
    "# The input for BERT-based models always starts with the classification token \n",
    "# representing the whole sequence and ends with the separator token \n",
    "# marking the end of the sequence.\n",
    "text = \"[CLS] Hello I'm a [MASK] model. [SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(f\"Tokenized input: {tokenized_text}\")\n",
    "\n",
    "# get index of mask token in tokenized text for extracting predictions later\n",
    "masked_index = tokenized_text.index(\"[MASK]\")\n",
    "print(f\"Mask token is at position {masked_index}\")\n",
    "\n",
    "# convert tokens to IDs\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(f\"Token IDs: {indexed_tokens}\")\n",
    "\n",
    "# transform IDs to troch tensor datatype\n",
    "tokens_tensor = torch.tensor([indexed_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model predictions (= score distribution across all vocabulary tokens for each token)\n",
    "# we use no_grad becasue we want to use the model without training it further \n",
    "# (i.e., computing gradients)\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    print(f\"Model output:\\n{outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = outputs.logits\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform scores into a probability distribution for the mask token using softmax\n",
    "# see https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch-nn-functional-softmax\n",
    "# for details\n",
    "print(f\"Shape of [MASK] token vector {predictions[0, masked_index].shape}\")\n",
    "probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "\n",
    "# extract the top 5 matches\n",
    "top_k_probs, top_k_indices = torch.topk(probs, 5, sorted=True)\n",
    "\n",
    "# print top 5 matches with their probabilities\n",
    "for token_prob, token_id in zip(top_k_probs, top_k_indices):\n",
    "    # convert token id back to token\n",
    "    # token_id is a tensor with one element, .item() extracts that element\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens(token_id.item())\n",
    "    print(f\"[MASK]: {predicted_token} | prob: {token_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Sample a continuation of a given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text continuation sampling from autoregressive (causal) language models (GPT-2 & co.)\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator(\"This is the beginning of a very thrilling story. One day,\", max_length=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening within this pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "\n",
    "# Generation requires sampling, set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is the beginning of a very thrilling story. One day,\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(f\"Tokenized input: {tokenized_text}\")\n",
    "# the GPT tokenizers apply subword splitting (to increase vocabulary coverage) \n",
    "# and each beginning of a token is marked with the special character Ä \n",
    "\n",
    "# convert tokens to IDs\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(f\"Token IDs: {indexed_tokens}\")\n",
    "\n",
    "# transform IDs to troch tensor datatype\n",
    "encoded_input = torch.tensor([indexed_tokens])\n",
    "\n",
    "# short version of tokenize + convert_to_ids + transform into torch tensor \n",
    "# + create attention mask (required for padding when processing several sequences of different length at once)\n",
    "#encoded_input = tokenizer(text, return_tensors='pt')\n",
    "#print(f\"Encoded Input:\\n{encoded_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate() repeatedly samples from token distributions, see \n",
    "# https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate\n",
    "# for different strategies (the parameters can also be passed into the pipeline above)\n",
    "tokens= model.generate(encoded_input, max_length=40, do_sample=True)\n",
    "\n",
    "output = tokenizer.decode(tokens.squeeze()) # squeeze removes the automatically added batch dimension (which is 1 in our case as we only have one input sequence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "### 2. Using fine-tuned models for text classification\n",
    "* given a sentence (or segement), predict a label for it\n",
    "* possible labels: \n",
    " * sentiment (positive/negative/neutral), \n",
    " * paraphrase (yes/no)\n",
    " * **grammatical acceptability (acceptable/unacceptable)**\n",
    " * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [CoLA: Corpus of Linguistic Acceptability](https://aclanthology.org/Q19-1040/)\n",
    "* collection of acceptable and unacceptable linguistic examples\n",
    "* extracted from syntax text books\n",
    "* used to train models on predicting grammatical acceptability\n",
    "* [several fine-tuned versions of models](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=cola) available through huggingface\n",
    "    * select one based on model card/popularity/Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Here we have a special case:\n",
    "# We want to use a user-built model that was trained using the tensorflow backend, \n",
    "# so we need to specify the from_tf flag to tell pytorch to import the weights accordingly\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(\"ccsobral/distilbert-base-uncased-finetuned-cola\", from_tf=True)\n",
    "classification_tokenizer = AutoTokenizer.from_pretrained(\"ccsobral/distilbert-base-uncased-finetuned-cola\")\n",
    "\n",
    "# We can also explicitely pass a model and tokenizer into the pipeline (instead of just the identifier as above)\n",
    "classifier = pipeline(\"text-classification\", model=classification_model, tokenizer=classification_tokenizer)\n",
    "print(classifier(\"This is a grammatical sentence.\"))\n",
    "print(classifier(\"This grammatical is sentence a.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening within this pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ccsobral/distilbert-base-uncased-finetuned-cola\")\n",
    "# This model was pre-trained using the tensorflow backend, so we need to specify the \n",
    "# from_tf flag to tell pytorch to import the weights accordingly\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ccsobral/distilbert-base-uncased-finetuned-cola\", from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"This is a grammatical sentence.\"\n",
    "#input_sentence = \"This grammatical is sentence a.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "# feed the input into the model and extract the calculated scores at the output layer\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# convert into probabilities\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "print(f\"Label probabilities: {probs}\")\n",
    "\n",
    "# get the maximum score\n",
    "predicted_class_id = probs.argmax().item()\n",
    "print(f\"Predicted class ID: {predicted_class_id}\")\n",
    "# for training the model, classes were mapped to ids as well\n",
    "# this mapping is stored in the model.config\n",
    "print(f\"Model class mapping: {model.config.id2label}\")\n",
    "# so we can transform this back into the label\n",
    "print(f\"label: {model.config.id2label[predicted_class_id]}, score: {probs.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [Huggingface documentation on how to fine-tune a model yourself](https://huggingface.co/docs/transformers/training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### 3. Using signals from pre-trained models\n",
    "#### a. Calculating surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I like my coffe with cream and\"\n",
    "\n",
    "model_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "next_words = [\"Ä sugar\", \"Ä honey\", \"Ä dog\"]\n",
    "next_word_ids = [tokenizer.convert_tokens_to_ids(word) for word in next_words]\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = model(**model_input)\n",
    "\n",
    "predictions = outputs.logits\n",
    "# apply softmax on last position to get probability distribution for next word\n",
    "probs = torch.nn.functional.softmax(predictions[0, -1], dim=-1)\n",
    "# compute surprisals \n",
    "surprisals = -torch.log(probs)\n",
    "\n",
    "for word, word_id in zip(next_words, next_word_ids):\n",
    "    print(f\"{word}\\n\\tProbability: {probs[word_id]}\\n\\tSurprisal: {surprisals[word_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Extracting embeddings, hidden representations and attention weights\n",
    "This goes beyond the scope of this introduction, but if you are interested in what other information one can extract from pre-trained models, you'll find the documentation [here](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/output#model-outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
